# Analyticsæ¨¡å—æŠ€æœ¯æ–‡æ¡£

## ğŸ“š æ¨¡å—æ¦‚è¿°

Analyticsæ¨¡å—æ˜¯æ™ºèƒ½å­¦ä¹ ç³»ç»Ÿçš„æ ¸å¿ƒåˆ†æå¼•æ“ï¼Œå®ç°äº†åŸºäºæ§åˆ¶ç†è®ºçš„äºº-LLMèåˆæ•™å­¦æ¡†æ¶ã€‚æ¨¡å—åŒ…å«è¡Œä¸ºæ•°æ®é‡‡é›†ã€å­¦ä¹ è€…ç”»åƒæ„å»ºã€çŸ¥è¯†çŠ¶æ€è¿½è¸ªã€æ™ºèƒ½çŠ¶æ€é¢„æµ‹ç­‰å…³é”®åŠŸèƒ½ã€‚

## ğŸ—ï¸ æ¶æ„æ€»è§ˆ

```
analytics/
â”œâ”€â”€ behavior_logger.py      # è¡Œä¸ºæ•°æ®é‡‡é›†ä¸ç‰¹å¾æå–
â”œâ”€â”€ improved_student_model.py  # å¢å¼ºå­¦ä¹ è€…ç”»åƒæ„å»º
â”œâ”€â”€ bayesian_kt.py          # è´å¶æ–¯çŸ¥è¯†è¿½è¸ªç®—æ³•  
â”œâ”€â”€ ml_state_predictor.py   # æœºå™¨å­¦ä¹ çŠ¶æ€é¢„æµ‹
â”œâ”€â”€ quiz_generator.py       # è‡ªé€‚åº”å‡ºé¢˜ä¸è¯„ä¼°
â”œâ”€â”€ single_user_model.py    # å•ç”¨æˆ·æ·±åº¦å»ºæ¨¡
â”œâ”€â”€ api_integration.py      # APIé›†æˆä¸æœåŠ¡å°è£…
â””â”€â”€ offline_evaluator.py    # ç¦»çº¿è¯„ä¼°ä¸éªŒè¯
```

## ğŸ“Š 1. è¡Œä¸ºæ•°æ®é‡‡é›†æ¨¡å— (behavior_logger.py)

### 1.1 æ ¸å¿ƒåŠŸèƒ½
è´Ÿè´£å®æ—¶é‡‡é›†ã€å­˜å‚¨å’Œåˆ†æå­¦ä¹ è€…çš„ç¼–ç¨‹è¡Œä¸ºæ•°æ®ï¼Œä½œä¸ºæ•´ä¸ªæ™ºèƒ½ç³»ç»Ÿçš„"ä¼ æ„Ÿå™¨"å±‚ã€‚

### 1.2 æ•°æ®æ¨¡å‹

#### BehaviorEvent æ•°æ®ç»“æ„
```python
@dataclass
class BehaviorEvent:
    timestamp: float           # äº‹ä»¶æ—¶é—´æˆ³
    student_id: str           # å­¦ä¹ è€…ID
    session_id: str           # ä¼šè¯ID
    event_type: EventType     # äº‹ä»¶ç±»å‹æšä¸¾
    
    # åŸºç¡€å±æ€§
    duration: Optional[float] = None          # äº‹ä»¶æŒç»­æ—¶é—´
    
    # ä»£ç ç¼–è¾‘ç›¸å…³
    code_before: Optional[str] = None         # ç¼–è¾‘å‰ä»£ç 
    code_after: Optional[str] = None          # ç¼–è¾‘åä»£ç 
    cursor_position: Optional[Dict] = None    # å…‰æ ‡ä½ç½®
    edit_length: Optional[int] = None         # ç¼–è¾‘é•¿åº¦
    
    # é”™è¯¯ç›¸å…³
    error_type: Optional[str] = None          # é”™è¯¯ç±»å‹
    error_message: Optional[str] = None       # é”™è¯¯ä¿¡æ¯
    
    # äº¤äº’ç›¸å…³
    help_query: Optional[str] = None          # æ±‚åŠ©å†…å®¹
    ai_response: Optional[str] = None         # AIå›åº”
    
    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    current_task: Optional[str] = None        # å½“å‰ä»»åŠ¡
    knowledge_points: Optional[List[str]] = None  # æ¶‰åŠçŸ¥è¯†ç‚¹
    metadata: Optional[Dict] = None           # é¢å¤–å…ƒæ•°æ®
```

#### EventType äº‹ä»¶ç±»å‹
```python
class EventType(Enum):
    CODE_EDIT = "code_edit"           # ä»£ç ç¼–è¾‘
    ERROR_OCCUR = "error_occur"       # é”™è¯¯å‘ç”Ÿ
    ERROR_FIX = "error_fix"           # é”™è¯¯ä¿®å¤
    HELP_REQUEST = "help_request"     # æ±‚åŠ©è¯·æ±‚
    PAUSE_START = "pause_start"       # æš‚åœå¼€å§‹
    PAUSE_END = "pause_end"           # æš‚åœç»“æŸ
    TASK_START = "task_start"         # ä»»åŠ¡å¼€å§‹
    TASK_COMPLETE = "task_complete"   # ä»»åŠ¡å®Œæˆ
    CODE_RUN = "code_run"             # ä»£ç è¿è¡Œ
    CODE_PASTE = "code_paste"         # ä»£ç ç²˜è´´
    USER_INTERACTION = "user_interaction"  # ä¸€èˆ¬äº¤äº’
```

### 1.3 æ ¸å¿ƒç®—æ³•

#### å®æ—¶ç‰¹å¾æå–ç®—æ³•
```python
def extract_behavioral_features(self, session_id: str, 
                               time_window: int = 300) -> Dict[str, float]:
    """
    æå–æŒ‡å®šæ—¶é—´çª—å£å†…çš„è¡Œä¸ºç‰¹å¾
    
    ç®—æ³•æ€æƒ³ï¼š
    1. æ—¶é—´çª—å£æ»‘åŠ¨åˆ†æï¼šè·å–æœ€è¿‘5åˆ†é’Ÿçš„è¡Œä¸ºäº‹ä»¶
    2. å¤šç»´åº¦ç‰¹å¾è®¡ç®—ï¼šç¼–è¾‘é¢‘ç‡ã€é”™è¯¯æ¨¡å¼ã€äº¤äº’æ¨¡å¼
    3. è®¤çŸ¥è´Ÿè·æ¨æ–­ï¼šåŸºäºè¡Œä¸ºå¯†åº¦å’Œé”™è¯¯ç‡
    """
    
    current_time = time.time()
    window_start = current_time - time_window
    
    # è·å–æ—¶é—´çª—å£å†…çš„äº‹ä»¶
    events = self._get_events_in_window(session_id, window_start, current_time)
    
    # è®¡ç®—åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features = {
        # ç¼–è¾‘è¡Œä¸ºç‰¹å¾
        'edit_frequency': self._calculate_edit_frequency(events),
        'avg_edit_length': self._calculate_avg_edit_length(events),
        'edit_variance': self._calculate_edit_variance(events),
        
        # é”™è¯¯è¡Œä¸ºç‰¹å¾  
        'error_rate': self._calculate_error_rate(events),
        'error_fix_time': self._calculate_avg_error_fix_time(events),
        'consecutive_errors': self._calculate_consecutive_errors(events),
        
        # æš‚åœè¡Œä¸ºç‰¹å¾
        'pause_frequency': self._calculate_pause_frequency(events),
        'avg_pause_duration': self._calculate_avg_pause_duration(events),
        'total_active_time': self._calculate_active_time(events),
        
        # æ±‚åŠ©è¡Œä¸ºç‰¹å¾
        'help_request_rate': self._calculate_help_rate(events),
        'help_response_satisfaction': self._calculate_help_satisfaction(events),
        
        # ç»¼åˆç‰¹å¾
        'activity_ratio': self._calculate_activity_ratio(events),
        'focus_score': self._calculate_focus_score(events),
        'progress_velocity': self._calculate_progress_velocity(events)
    }
    
    return features
```

#### è®¤çŸ¥è´Ÿè·æ¨æ–­ç®—æ³•
```python
def infer_cognitive_load(self, behavioral_features: Dict[str, float]) -> str:
    """
    åŸºäºè¡Œä¸ºç‰¹å¾æ¨æ–­è®¤çŸ¥è´Ÿè·ç­‰çº§
    
    ç®—æ³•ä¾æ®ï¼š
    1. é«˜é¢‘ç¼–è¾‘ + é«˜é”™è¯¯ç‡ â†’ é«˜è®¤çŸ¥è´Ÿè·
    2. é¢‘ç¹æš‚åœ + æ±‚åŠ© â†’ é«˜è®¤çŸ¥è´Ÿè·  
    3. ç¨³å®šç¼–è¾‘ + ä½é”™è¯¯ç‡ â†’ ä½è®¤çŸ¥è´Ÿè·
    """
    
    # æƒé‡åŒ–è¯„åˆ†
    load_score = 0.0
    
    # é”™è¯¯ç‡æƒé‡ (0.3)
    error_contribution = min(behavioral_features.get('error_rate', 0) * 3, 1.0) * 0.3
    load_score += error_contribution
    
    # ç¼–è¾‘æ–¹å·®æƒé‡ (0.25) - ç¼–è¾‘ä¸ç¨³å®šè¡¨ç¤ºå›°æƒ‘
    edit_variance = behavioral_features.get('edit_variance', 0)
    variance_contribution = min(edit_variance / 100, 1.0) * 0.25
    load_score += variance_contribution
    
    # æš‚åœé¢‘ç‡æƒé‡ (0.2)
    pause_contribution = min(behavioral_features.get('pause_frequency', 0) / 10, 1.0) * 0.2
    load_score += pause_contribution
    
    # æ±‚åŠ©é¢‘ç‡æƒé‡ (0.15)
    help_contribution = min(behavioral_features.get('help_request_rate', 0) * 5, 1.0) * 0.15
    load_score += help_contribution
    
    # ä¸“æ³¨åº¦åå‘æƒé‡ (0.1)
    focus_penalty = (1 - behavioral_features.get('focus_score', 0.5)) * 0.1
    load_score += focus_penalty
    
    # ç­‰çº§åˆ¤å®š
    if load_score >= 0.7:
        return "high"
    elif load_score >= 0.4:
        return "medium"
    else:
        return "low"
```

## ğŸ§  2. å­¦ä¹ è€…ç”»åƒæ„å»º (improved_student_model.py)

### 2.1 æ ¸å¿ƒæ¦‚å¿µ
æ„å»ºå¤šç»´åº¦ã€åŠ¨æ€æ›´æ–°çš„å­¦ä¹ è€…æ•°å­—ç”»åƒï¼ŒåŒ…å«è®¤çŸ¥çŠ¶æ€ã€æƒ…æ„ŸçŠ¶æ€ã€å­¦ä¹ åå¥½ç­‰å…³é”®ç»´åº¦ã€‚

### 2.2 ç”»åƒæ•°æ®ç»“æ„

#### è®¤çŸ¥çŠ¶æ€æ¨¡å‹
```python
@dataclass  
class CognitiveState:
    # çŸ¥è¯†æ°´å¹³
    knowledge_level: float = 1.0              # æ•´ä½“çŸ¥è¯†æ°´å¹³ (1-5)
    knowledge_confidence: float = 0.3         # çŸ¥è¯†æ°´å¹³ç½®ä¿¡åº¦
    
    # è®¤çŸ¥è´Ÿè·
    cognitive_load: str = "medium"            # å½“å‰è®¤çŸ¥è´Ÿè·ç­‰çº§
    cognitive_load_score: float = 0.5         # è®¤çŸ¥è´Ÿè·åˆ†æ•° (0-1)
    cognitive_load_confidence: float = 0.5    # è®¤çŸ¥è´Ÿè·ç½®ä¿¡åº¦
    
    # å›°æƒ‘ç¨‹åº¦
    confusion_level: str = "none"             # å›°æƒ‘ç­‰çº§
    confusion_score: float = 0.0              # å›°æƒ‘åˆ†æ•° (0-1)
    confusion_confidence: float = 0.5         # å›°æƒ‘ç½®ä¿¡åº¦
    
    # çŸ¥è¯†ç‚¹æŒæ¡
    knowledge_points: Dict[str, KnowledgePoint] = field(default_factory=dict)
```

#### æƒ…æ„ŸçŠ¶æ€æ¨¡å‹
```python
@dataclass
class EmotionalState:
    # æŒ«è´¥æ„Ÿ
    frustration_level: str = "none"           # æŒ«è´¥ç­‰çº§
    frustration_score: float = 0.0            # æŒ«è´¥åˆ†æ•° (0-1)
    frustration_confidence: float = 0.5       # æŒ«è´¥ç½®ä¿¡åº¦
    
    # ä¸“æ³¨åº¦
    focus_level: str = "medium"               # ä¸“æ³¨ç­‰çº§
    focus_score: float = 0.5                  # ä¸“æ³¨åˆ†æ•° (0-1)
    focus_confidence: float = 0.5             # ä¸“æ³¨ç½®ä¿¡åº¦
```

#### å­¦ä¹ åå¥½æ¨¡å‹
```python
@dataclass
class LearningPreferences:
    main_preference: str = "code_examples"    # ä¸»è¦å­¦ä¹ åå¥½
    
    # å„ç§åå¥½æƒé‡
    preferences: Dict[str, float] = field(default_factory=lambda: {
        "code_examples": 0.2,     # ä»£ç ç¤ºä¾‹
        "text_explanations": 0.2,  # æ–‡å­—è§£é‡Š
        "analogies": 0.2,         # ç±»æ¯”è¯´æ˜
        "visual_aids": 0.2,       # å¯è§†åŒ–è¾…åŠ©
        "interactive": 0.2        # äº¤äº’å¼å­¦ä¹ 
    })
    
    # åå¥½ç½®ä¿¡åº¦
    preference_confidence: Dict[str, float] = field(default_factory=lambda: {
        "code_examples": 0.1,
        "text_explanations": 0.1,
        "analogies": 0.1,
        "visual_aids": 0.1,
        "interactive": 0.1
    })
```

### 2.3 ç”»åƒæ›´æ–°ç®—æ³•

#### åŠ¨æ€çŠ¶æ€æ›´æ–°ç®—æ³•
```python
def update_from_behavior_data(self, student_id: str, session_id: str) -> None:
    """
    åŸºäºè¡Œä¸ºæ•°æ®åŠ¨æ€æ›´æ–°å­¦ä¹ è€…ç”»åƒ
    
    ç®—æ³•æµç¨‹ï¼š
    1. è·å–æœ€æ–°è¡Œä¸ºç‰¹å¾
    2. å¤šç»´åº¦çŠ¶æ€æ¨æ–­
    3. å¢é‡å¼ç”»åƒæ›´æ–°
    4. ç½®ä¿¡åº¦è°ƒæ•´
    """
    
    # 1. è·å–è¡Œä¸ºç‰¹å¾
    behavioral_features = self.behavior_logger.extract_behavioral_features(session_id)
    
    # 2. è®¤çŸ¥çŠ¶æ€æ¨æ–­
    self._update_cognitive_state(student_id, behavioral_features)
    
    # 3. æƒ…æ„ŸçŠ¶æ€æ¨æ–­  
    self._update_emotional_state(student_id, behavioral_features)
    
    # 4. å­¦ä¹ åå¥½æ›´æ–°
    self._update_learning_preferences(student_id, behavioral_features)
    
    # 5. æ•´ä½“ç½®ä¿¡åº¦è°ƒæ•´
    self._adjust_overall_confidence(student_id, behavioral_features)

def _update_cognitive_state(self, student_id: str, features: Dict[str, float]) -> None:
    """è®¤çŸ¥çŠ¶æ€æ›´æ–°ç®—æ³•"""
    model = self.get_model(student_id)
    
    # è®¤çŸ¥è´Ÿè·æ›´æ–° - åŸºäºå¤šç»´åº¦ç‰¹å¾èåˆ
    current_load = self.behavior_logger.infer_cognitive_load(features)
    
    # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–° (Î±=0.3)
    Î± = 0.3
    old_score = self._load_to_score(model.cognitive_state.cognitive_load)
    new_score = self._load_to_score(current_load)
    updated_score = Î± * new_score + (1 - Î±) * old_score
    
    model.cognitive_state.cognitive_load = self._score_to_load(updated_score)
    model.cognitive_state.cognitive_load_score = updated_score
    
    # å›°æƒ‘ç¨‹åº¦æ›´æ–° - åŸºäºé”™è¯¯æ¨¡å¼å’Œæš‚åœè¡Œä¸º
    confusion_indicators = [
        features.get('consecutive_errors', 0) / 5,      # è¿ç»­é”™è¯¯
        features.get('pause_frequency', 0) / 10,        # æš‚åœé¢‘ç‡
        features.get('help_request_rate', 0) * 5,       # æ±‚åŠ©é¢‘ç‡
        1 - features.get('progress_velocity', 0.5)      # è¿›åº¦ç¼“æ…¢
    ]
    
    confusion_score = min(np.mean(confusion_indicators), 1.0)
    model.cognitive_state.confusion_score = Î± * confusion_score + (1 - Î±) * model.cognitive_state.confusion_score
    model.cognitive_state.confusion_level = self._score_to_confusion_level(model.cognitive_state.confusion_score)
```

#### å­¦ä¹ åå¥½æ¨æ–­ç®—æ³•
```python
def _infer_learning_preference(self, interaction_history: List[Dict]) -> Dict[str, float]:
    """
    åŸºäºäº¤äº’å†å²æ¨æ–­å­¦ä¹ åå¥½
    
    ç®—æ³•æ€æƒ³ï¼š
    1. åˆ†æç”¨æˆ·å¯¹ä¸åŒç±»å‹å†…å®¹çš„ååº”
    2. ç»Ÿè®¡æœ‰æ•ˆäº¤äº’çš„ç±»å‹åˆ†å¸ƒ
    3. è´å¶æ–¯æ›´æ–°åå¥½æƒé‡
    """
    
    preference_scores = {
        "code_examples": 0.0,
        "text_explanations": 0.0, 
        "analogies": 0.0,
        "visual_aids": 0.0,
        "interactive": 0.0
    }
    
    total_interactions = len(interaction_history)
    
    for interaction in interaction_history:
        content_type = interaction.get('content_type', 'unknown')
        effectiveness = interaction.get('effectiveness_score', 0.5)  # 0-1
        
        if content_type in preference_scores:
            # åŠ æƒç´¯ç§¯ï¼šæ•ˆæœå¥½çš„å†…å®¹ç±»å‹æƒé‡æ›´é«˜
            preference_scores[content_type] += effectiveness
    
    # å½’ä¸€åŒ–å¤„ç†
    if total_interactions > 0:
        for key in preference_scores:
            preference_scores[key] /= total_interactions
    
    # Softmaxå½’ä¸€åŒ–ç¡®ä¿å’Œä¸º1
    scores_array = np.array(list(preference_scores.values()))
    softmax_scores = np.exp(scores_array) / np.sum(np.exp(scores_array))
    
    return dict(zip(preference_scores.keys(), softmax_scores))
```

## ğŸ“ˆ 3. è´å¶æ–¯çŸ¥è¯†è¿½è¸ª (bayesian_kt.py)

### 3.1 ç®—æ³•åŸç†
å®ç°æ ‡å‡†çš„è´å¶æ–¯çŸ¥è¯†è¿½è¸ª(BKT)ç®—æ³•ï¼Œç”¨äºç²¾ç¡®è·Ÿè¸ªå­¦ä¹ è€…å¯¹å„çŸ¥è¯†ç‚¹çš„æŒæ¡çŠ¶æ€ã€‚

### 3.2 BKTæ•°å­¦æ¨¡å‹

#### æ ¸å¿ƒå‚æ•°
```python
@dataclass
class BKTParameters:
    P_L0: float = 0.1    # åˆå§‹æŒæ¡æ¦‚ç‡ (Prior Knowledge)
    P_T: float = 0.2     # å­¦ä¹ è½¬ç§»æ¦‚ç‡ (Learning Rate)  
    P_G: float = 0.25    # çŒœæµ‹æ¦‚ç‡ (Guess Rate)
    P_S: float = 0.1     # å¤±è¯¯æ¦‚ç‡ (Slip Rate)
```

#### çŠ¶æ€æ›´æ–°å…¬å¼
```python
def update_mastery(self, observation: LearningObservation) -> float:
    """
    BKTæ ¸å¿ƒæ›´æ–°ç®—æ³•
    
    æ•°å­¦å…¬å¼ï¼š
    P(L_n+1) = P(L_n|evidence) + P(T) * (1 - P(L_n|evidence))
    
    å…¶ä¸­ï¼š
    P(L_n|evidence) = P(evidence|L_n) * P(L_n) / P(evidence)
    """
    
    # 1. è®¡ç®—è§‚å¯Ÿæ¦‚ç‡
    if observation.correct:
        # ç­”å¯¹çš„æƒ…å†µ
        p_correct_given_mastery = 1 - self.params.P_S
        p_correct_given_no_mastery = self.params.P_G
    else:
        # ç­”é”™çš„æƒ…å†µ  
        p_correct_given_mastery = self.params.P_S
        p_correct_given_no_mastery = 1 - self.params.P_G
    
    # 2. è´å¶æ–¯åéªŒæ›´æ–°
    numerator = p_correct_given_mastery * self.current_mastery_prob
    denominator = (p_correct_given_mastery * self.current_mastery_prob + 
                  p_correct_given_no_mastery * (1 - self.current_mastery_prob))
    
    if denominator > 0:
        posterior_mastery = numerator / denominator
    else:
        posterior_mastery = self.current_mastery_prob
    
    # 3. å­¦ä¹ è½¬ç§»æ›´æ–°
    self.current_mastery_prob = posterior_mastery + self.params.P_T * (1 - posterior_mastery)
    
    # 4. è®°å½•è½¨è¿¹
    self.trajectory.append({
        'timestamp': observation.timestamp,
        'observation': observation.correct,
        'prior': self.current_mastery_prob,
        'posterior': posterior_mastery,
        'updated': self.current_mastery_prob
    })
    
    return self.current_mastery_prob
```

### 3.3 å‚æ•°ä¼°è®¡ç®—æ³•

#### æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)
```python
def estimate_parameters(self, observations: List[LearningObservation]) -> BKTParameters:
    """
    ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¼˜åŒ–BKTå‚æ•°
    
    ç®—æ³•ï¼šEMç®—æ³•è¿­ä»£ä¼˜åŒ–
    1. Eæ­¥ï¼šè®¡ç®—éšçŠ¶æ€æœŸæœ›
    2. Mæ­¥ï¼šæ›´æ–°å‚æ•°
    """
    
    # åˆå§‹å‚æ•°
    params = BKTParameters()
    
    for iteration in range(self.max_iterations):
        # Eæ­¥ï¼šå‰å‘-åå‘ç®—æ³•è®¡ç®—çŠ¶æ€æ¦‚ç‡
        forward_probs, backward_probs = self._forward_backward(observations, params)
        
        # Mæ­¥ï¼šæ›´æ–°å‚æ•°
        new_params = self._maximize_parameters(observations, forward_probs, backward_probs)
        
        # æ”¶æ•›æ£€æŸ¥
        if self._parameters_converged(params, new_params):
            break
            
        params = new_params
    
    return params

def _forward_backward(self, observations: List[LearningObservation], 
                     params: BKTParameters) -> Tuple[np.ndarray, np.ndarray]:
    """å‰å‘-åå‘ç®—æ³•è®¡ç®—çŠ¶æ€æ¦‚ç‡"""
    
    n = len(observations)
    forward = np.zeros((n + 1, 2))  # [æ—¶é—´, çŠ¶æ€] çŠ¶æ€: 0=æœªæŒæ¡, 1=å·²æŒæ¡
    backward = np.zeros((n + 1, 2))
    
    # å‰å‘ä¼ æ’­
    forward[0, 0] = 1 - params.P_L0  # åˆå§‹æœªæŒæ¡æ¦‚ç‡
    forward[0, 1] = params.P_L0      # åˆå§‹æŒæ¡æ¦‚ç‡
    
    for t in range(n):
        obs = observations[t]
        
        # è§‚å¯Ÿæ¦‚ç‡
        if obs.correct:
            p_obs_given_not_mastered = params.P_G
            p_obs_given_mastered = 1 - params.P_S
        else:
            p_obs_given_not_mastered = 1 - params.P_G
            p_obs_given_mastered = params.P_S
        
        # çŠ¶æ€è½¬ç§»
        forward[t+1, 0] = (forward[t, 0] * (1 - params.P_T) * p_obs_given_not_mastered)
        forward[t+1, 1] = (forward[t, 0] * params.P_T * p_obs_given_mastered + 
                          forward[t, 1] * 1.0 * p_obs_given_mastered)
        
        # å½’ä¸€åŒ–
        total = forward[t+1, 0] + forward[t+1, 1]
        if total > 0:
            forward[t+1, 0] /= total
            forward[t+1, 1] /= total
    
    # åå‘ä¼ æ’­
    backward[n, 0] = backward[n, 1] = 1.0
    
    for t in range(n-1, -1, -1):
        obs = observations[t]
        
        if obs.correct:
            p_obs_given_not_mastered = params.P_G
            p_obs_given_mastered = 1 - params.P_S
        else:
            p_obs_given_not_mastered = 1 - params.P_G
            p_obs_given_mastered = params.P_S
        
        backward[t, 0] = ((1 - params.P_T) * p_obs_given_not_mastered * backward[t+1, 0] + 
                         params.P_T * p_obs_given_mastered * backward[t+1, 1])
        backward[t, 1] = (1.0 * p_obs_given_mastered * backward[t+1, 1])
    
    return forward, backward
```

## ğŸ¤– 4. æœºå™¨å­¦ä¹ çŠ¶æ€é¢„æµ‹ (ml_state_predictor.py)

### 4.1 ç‰¹å¾å·¥ç¨‹

#### 19ç»´ç‰¹å¾å‘é‡è®¾è®¡
```python
@dataclass
class FeatureVector:
    """æœºå™¨å­¦ä¹ ç‰¹å¾å‘é‡ - 19ç»´"""
    
    # ç¼–è¾‘è¡Œä¸ºç‰¹å¾ (5ç»´)
    edit_frequency: float = 0.0          # ç¼–è¾‘é¢‘ç‡ (æ¬¡/åˆ†é’Ÿ)
    avg_edit_length: float = 0.0         # å¹³å‡ç¼–è¾‘é•¿åº¦
    edit_variance: float = 0.0           # ç¼–è¾‘é•¿åº¦æ–¹å·®
    code_completion_ratio: float = 0.0   # ä»£ç å®Œæˆåº¦
    syntax_correctness: float = 0.0      # è¯­æ³•æ­£ç¡®æ€§
    
    # é”™è¯¯è¡Œä¸ºç‰¹å¾ (4ç»´)  
    error_rate: float = 0.0              # é”™è¯¯ç‡
    avg_error_fix_time: float = 0.0      # å¹³å‡é”™è¯¯ä¿®å¤æ—¶é—´
    consecutive_errors: float = 0.0       # è¿ç»­é”™è¯¯æ¬¡æ•°
    error_type_diversity: float = 0.0    # é”™è¯¯ç±»å‹å¤šæ ·æ€§
    
    # æ—¶é—´è¡Œä¸ºç‰¹å¾ (3ç»´)
    total_active_time: float = 0.0       # æ€»æ´»è·ƒæ—¶é—´
    pause_frequency: float = 0.0         # æš‚åœé¢‘ç‡
    avg_pause_duration: float = 0.0      # å¹³å‡æš‚åœæ—¶é•¿
    
    # äº¤äº’è¡Œä¸ºç‰¹å¾ (3ç»´)
    help_request_rate: float = 0.0       # æ±‚åŠ©é¢‘ç‡
    copy_paste_frequency: float = 0.0    # å¤åˆ¶ç²˜è´´é¢‘ç‡
    ui_interaction_rate: float = 0.0     # UIäº¤äº’é¢‘ç‡
    
    # è¿›åº¦è¡Œä¸ºç‰¹å¾ (2ç»´)
    task_completion_rate: float = 0.0    # ä»»åŠ¡å®Œæˆç‡
    learning_velocity: float = 0.0       # å­¦ä¹ é€Ÿåº¦
    
    # ä¸“æ³¨åº¦ç‰¹å¾ (2ç»´)
    focus_score: float = 0.0             # ä¸“æ³¨åº¦åˆ†æ•°
    context_switch_rate: float = 0.0     # ä¸Šä¸‹æ–‡åˆ‡æ¢ç‡
```

#### ç‰¹å¾æå–ç®—æ³•
```python
def extract_features_from_behavior(self, behavior_data: Dict[str, Any],
                                 session_summary: Dict[str, Any]) -> FeatureVector:
    """
    ä»è¡Œä¸ºæ•°æ®ä¸­æå–æœºå™¨å­¦ä¹ ç‰¹å¾
    
    ç®—æ³•æ€æƒ³ï¼š
    1. å¤šæ—¶é—´çª—å£èšåˆ
    2. ç»Ÿè®¡ç‰¹å¾è®¡ç®—
    3. å½’ä¸€åŒ–å¤„ç†
    """
    
    features = FeatureVector()
    
    # 1. ç¼–è¾‘è¡Œä¸ºç‰¹å¾æå–
    edit_events = self._filter_events_by_type(behavior_data, 'code_edit')
    if edit_events:
        features.edit_frequency = len(edit_events) / session_summary.get('duration_minutes', 1)
        edit_lengths = [e.get('edit_length', 0) for e in edit_events]
        features.avg_edit_length = np.mean(edit_lengths)
        features.edit_variance = np.var(edit_lengths)
    
    # 2. é”™è¯¯è¡Œä¸ºç‰¹å¾æå–
    error_events = self._filter_events_by_type(behavior_data, 'error_occur')
    total_events = len(behavior_data.get('events', []))
    features.error_rate = len(error_events) / max(total_events, 1)
    
    # è®¡ç®—è¿ç»­é”™è¯¯
    features.consecutive_errors = self._calculate_max_consecutive_errors(error_events)
    
    # 3. æ—¶é—´è¡Œä¸ºç‰¹å¾æå–
    features.total_active_time = session_summary.get('active_time_minutes', 0)
    
    pause_events = self._filter_events_by_type(behavior_data, 'pause_end')
    if pause_events:
        features.pause_frequency = len(pause_events) / session_summary.get('duration_minutes', 1)
        pause_durations = [e.get('duration', 0) for e in pause_events]
        features.avg_pause_duration = np.mean(pause_durations)
    
    # 4. äº¤äº’è¡Œä¸ºç‰¹å¾æå–
    help_events = self._filter_events_by_type(behavior_data, 'help_request')
    features.help_request_rate = len(help_events) / max(total_events, 1)
    
    # 5. è¿›åº¦ç‰¹å¾è®¡ç®—
    features.task_completion_rate = session_summary.get('completion_rate', 0.0)
    features.learning_velocity = self._calculate_learning_velocity(behavior_data)
    
    # 6. ä¸“æ³¨åº¦ç‰¹å¾è®¡ç®—
    features.focus_score = self._calculate_focus_score(behavior_data)
    features.context_switch_rate = self._calculate_context_switches(behavior_data)
    
    return features

def _calculate_learning_velocity(self, behavior_data: Dict[str, Any]) -> float:
    """
    è®¡ç®—å­¦ä¹ é€Ÿåº¦
    
    ç®—æ³•ï¼šåŸºäºä»»åŠ¡è¿›åº¦å’Œæ—¶é—´çš„æ¯”å€¼
    """
    events = behavior_data.get('events', [])
    if not events:
        return 0.0
    
    # æ‰¾åˆ°ä»»åŠ¡å¼€å§‹å’Œå®Œæˆäº‹ä»¶
    start_events = [e for e in events if e.get('event_type') == 'task_start']
    complete_events = [e for e in events if e.get('event_type') == 'task_complete']
    
    if not start_events or not complete_events:
        return 0.0
    
    # è®¡ç®—å¹³å‡ä»»åŠ¡å®Œæˆæ—¶é—´
    avg_completion_time = np.mean([
        complete['timestamp'] - start['timestamp'] 
        for start, complete in zip(start_events, complete_events)
    ])
    
    # é€Ÿåº¦ = 1 / å¹³å‡å®Œæˆæ—¶é—´ (å½’ä¸€åŒ–)
    return 1.0 / (avg_completion_time / 60 + 1)  # è½¬æ¢ä¸ºåˆ†é’Ÿå¹¶é¿å…é™¤é›¶
```

### 4.2 æœºå™¨å­¦ä¹ æ¨¡å‹

#### Random Forestè®¤çŸ¥è´Ÿè·é¢„æµ‹å™¨
```python
class CognitiveLoadPredictor:
    """Random Forestè®¤çŸ¥è´Ÿè·é¢„æµ‹å™¨"""
    
    def __init__(self, n_estimators: int = 100):
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        self.feature_engineer = FeatureEngineer()
        self.label_encoder = LabelEncoder()
        self.confidence_threshold = 0.7
    
    def train(self, features: List[FeatureVector], labels: List[str]) -> Dict[str, float]:
        """
        è®­ç»ƒè®¤çŸ¥è´Ÿè·é¢„æµ‹æ¨¡å‹
        
        Args:
            features: ç‰¹å¾å‘é‡åˆ—è¡¨
            labels: è®¤çŸ¥è´Ÿè·æ ‡ç­¾ ['low', 'medium', 'high']
        
        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        if len(features) < 10:
            logger.warning("è®­ç»ƒæ ·æœ¬ä¸è¶³ï¼Œéœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬")
            return {'accuracy': 0.0, 'sample_size': len(features)}
        
        # ç‰¹å¾å·¥ç¨‹
        X = self.feature_engineer.fit_transform(features)
        y = self.label_encoder.fit_transform(labels)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æ¨¡å‹è®­ç»ƒ
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance = self.model.feature_importances_
        
        self.is_trained = True
        
        return {
            'accuracy': accuracy,
            'sample_size': len(features),
            'feature_importance': feature_importance.tolist()
        }
    
    def predict(self, feature: FeatureVector) -> PredictionResult:
        """
        é¢„æµ‹è®¤çŸ¥è´Ÿè·
        
        Returns:
            é¢„æµ‹ç»“æœåŒ…å«é¢„æµ‹å€¼å’Œç½®ä¿¡åº¦
        """
        if not self.is_trained:
            # ä½¿ç”¨åŸºäºè§„åˆ™çš„åå¤‡é¢„æµ‹
            return self._rule_based_prediction(feature)
        
        # ç‰¹å¾è½¬æ¢
        X = self.feature_engineer.transform([feature])
        
        # é¢„æµ‹æ¦‚ç‡
        proba = self.model.predict_proba(X)[0]
        prediction_idx = np.argmax(proba)
        confidence = proba[prediction_idx]
        
        # è½¬æ¢å›æ ‡ç­¾
        prediction = self.label_encoder.inverse_transform([prediction_idx])[0]
        
        return PredictionResult(
            prediction=prediction,
            confidence=float(confidence),
            raw_probabilities=proba.tolist()
        )
    
    def _rule_based_prediction(self, feature: FeatureVector) -> PredictionResult:
        """
        åŸºäºè§„åˆ™çš„åå¤‡é¢„æµ‹ç®—æ³•
        
        å½“MLæ¨¡å‹æœªè®­ç»ƒæ—¶ä½¿ç”¨çš„è§„åˆ™ç³»ç»Ÿ
        """
        score = 0.0
        
        # é”™è¯¯ç‡è´¡çŒ® (æƒé‡: 0.3)
        if feature.error_rate > 0.5:
            score += 0.3
        elif feature.error_rate > 0.3:
            score += 0.15
        
        # æš‚åœé¢‘ç‡è´¡çŒ® (æƒé‡: 0.25)
        if feature.pause_frequency > 5:
            score += 0.25
        elif feature.pause_frequency > 2:
            score += 0.125
        
        # æ±‚åŠ©é¢‘ç‡è´¡çŒ® (æƒé‡: 0.2)
        if feature.help_request_rate > 0.3:
            score += 0.2
        elif feature.help_request_rate > 0.1:
            score += 0.1
        
        # ç¼–è¾‘æ–¹å·®è´¡çŒ® (æƒé‡: 0.15)
        if feature.edit_variance > 100:
            score += 0.15
        elif feature.edit_variance > 50:
            score += 0.075
        
        # ä¸“æ³¨åº¦è´¡çŒ® (æƒé‡: 0.1)
        if feature.focus_score < 0.3:
            score += 0.1
        elif feature.focus_score < 0.6:
            score += 0.05
        
        # ç­‰çº§åˆ¤å®š
        if score >= 0.7:
            prediction = "high"
            confidence = 0.6
        elif score >= 0.4:
            prediction = "medium"  
            confidence = 0.7
        else:
            prediction = "low"
            confidence = 0.6
        
        return PredictionResult(
            prediction=prediction,
            confidence=confidence,
            raw_probabilities=[0.33, 0.33, 0.34]  # å‡åŒ€åˆ†å¸ƒä½œä¸ºé»˜è®¤
        )
```

#### å›°æƒ‘ç¨‹åº¦å›å½’é¢„æµ‹å™¨
```python
class ConfusionPredictor:
    """å›°æƒ‘ç¨‹åº¦å›å½’é¢„æµ‹å™¨"""
    
    def __init__(self, n_estimators: int = 100):
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=8,
            min_samples_split=5,
            min_samples_leaf=3,
            random_state=42
        )
        self.feature_engineer = FeatureEngineer()
        self.is_trained = False
    
    def train(self, features: List[FeatureVector], 
             confusion_scores: List[float]) -> Dict[str, float]:
        """
        è®­ç»ƒå›°æƒ‘ç¨‹åº¦é¢„æµ‹æ¨¡å‹
        
        Args:
            features: ç‰¹å¾å‘é‡åˆ—è¡¨
            confusion_scores: å›°æƒ‘åˆ†æ•°åˆ—è¡¨ (0-1)
        """
        if len(features) < 10:
            return {'mse': float('inf'), 'r2': 0.0, 'sample_size': len(features)}
        
        # ç‰¹å¾å·¥ç¨‹
        X = self.feature_engineer.fit_transform(features)
        y = np.array(confusion_scores)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # æ¨¡å‹è®­ç»ƒ
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°
        y_pred = self.model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        self.is_trained = True
        
        return {
            'mse': mse,
            'r2': r2,
            'sample_size': len(features)
        }
    
    def predict(self, feature: FeatureVector) -> PredictionResult:
        """é¢„æµ‹å›°æƒ‘ç¨‹åº¦åˆ†æ•°"""
        if not self.is_trained:
            return self._rule_based_confusion_prediction(feature)
        
        X = self.feature_engineer.transform([feature])
        confusion_score = self.model.predict(X)[0]
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        confusion_score = np.clip(confusion_score, 0.0, 1.0)
        
        # ç½®ä¿¡åº¦åŸºäºç‰¹å¾è´¨é‡
        confidence = self._calculate_prediction_confidence(feature)
        
        return PredictionResult(
            prediction=float(confusion_score),
            confidence=confidence,
            raw_probabilities=None
        )
    
    def _rule_based_confusion_prediction(self, feature: FeatureVector) -> PredictionResult:
        """åŸºäºè§„åˆ™çš„å›°æƒ‘ç¨‹åº¦é¢„æµ‹"""
        confusion_score = 0.0
        
        # è¿ç»­é”™è¯¯å¼ºçƒˆæŒ‡ç¤ºå›°æƒ‘
        if feature.consecutive_errors >= 3:
            confusion_score += 0.4
        elif feature.consecutive_errors >= 2:
            confusion_score += 0.2
        
        # é«˜é”™è¯¯ç‡æŒ‡ç¤ºå›°æƒ‘
        if feature.error_rate > 0.5:
            confusion_score += 0.3
        elif feature.error_rate > 0.3:
            confusion_score += 0.15
        
        # é¢‘ç¹æš‚åœæŒ‡ç¤ºå›°æƒ‘
        if feature.pause_frequency > 5:
            confusion_score += 0.2
        elif feature.pause_frequency > 2:
            confusion_score += 0.1
        
        # é¢‘ç¹æ±‚åŠ©æŒ‡ç¤ºå›°æƒ‘
        if feature.help_request_rate > 0.3:
            confusion_score += 0.1
        
        return PredictionResult(
            prediction=min(confusion_score, 1.0),
            confidence=0.6,
            raw_probabilities=None
        )
```

## ğŸ¯ 5. è‡ªé€‚åº”å‡ºé¢˜ç³»ç»Ÿ (quiz_generator.py)

### 5.1 é¢˜ç›®ç±»å‹è®¾è®¡

#### é¢˜ç›®æ•°æ®ç»“æ„
```python
@dataclass
class Question:
    id: str                              # é¢˜ç›®ID
    type: QuestionType                   # é¢˜ç›®ç±»å‹
    title: str                          # é¢˜ç›®æ ‡é¢˜
    content: str                        # é¢˜ç›®å†…å®¹
    knowledge_points: List[str]          # å…³è”çŸ¥è¯†ç‚¹
    difficulty: DifficultyLevel          # éš¾åº¦ç­‰çº§
    estimated_time: int                  # é¢„ä¼°å®Œæˆæ—¶é—´(åˆ†é’Ÿ)
    max_score: float                     # æœ€é«˜åˆ†æ•°
    
    # ç‰¹å®šé¢˜å‹çš„é¢å¤–æ•°æ®
    template: Optional[CodeTemplate] = None           # å¡«ç©ºé¢˜æ¨¡æ¿
    buggy_code: Optional[BuggyCode] = None           # é”™è¯¯çº æ­£é¢˜
    implementation_task: Optional[ImplementationTask] = None  # ç¼–ç¨‹å®ç°é¢˜
    
class QuestionType(Enum):
    FILL_IN_BLANK = "fill_in_blank"         # ä»£ç å¡«ç©º
    ERROR_CORRECTION = "error_correction"    # é”™è¯¯çº æ­£
    CODE_IMPLEMENTATION = "code_implementation"  # ä»£ç å®ç°
    CONCEPT_EXPLANATION = "concept_explanation"  # æ¦‚å¿µè§£é‡Š
    CODE_ANALYSIS = "code_analysis"         # ä»£ç åˆ†æ

class DifficultyLevel(Enum):
    EASY = "easy"           # ç®€å• (è®¤çŸ¥è´Ÿè·ä½æ—¶)
    MEDIUM = "medium"       # ä¸­ç­‰ (è®¤çŸ¥è´Ÿè·ä¸­ç­‰æ—¶)
    HARD = "hard"          # å›°éš¾ (è®¤çŸ¥è´Ÿè·ä½ä¸”æŒæ¡åº¦é«˜æ—¶)
```

### 5.2 è‡ªé€‚åº”å‡ºé¢˜ç®—æ³•

#### æ ¸å¿ƒç”Ÿæˆç®—æ³•
```python
def generate_adaptive_quiz(self, student_model_summary: Dict[str, Any],
                          target_knowledge_points: List[str],
                          num_questions: int = 3) -> List[Question]:
    """
    è‡ªé€‚åº”å‡ºé¢˜ç®—æ³•
    
    ç®—æ³•æµç¨‹ï¼š
    1. åˆ†æå­¦ä¹ è€…çŠ¶æ€
    2. ç¡®å®šé€‚åº”æ€§ç­–ç•¥
    3. é€‰æ‹©é¢˜ç›®ç±»å‹å’Œéš¾åº¦
    4. ç”Ÿæˆä¸ªæ€§åŒ–é¢˜ç›®
    """
    
    # 1. æå–å­¦ä¹ è€…çŠ¶æ€
    cognitive_state = student_model_summary.get('cognitive_state', {})
    cognitive_load = cognitive_state.get('cognitive_load', 'medium')
    confusion_level = cognitive_state.get('confusion_level', 'none')
    knowledge_level = cognitive_state.get('knowledge_level', 1.0)
    
    # 2. è‡ªé€‚åº”ç­–ç•¥å†³ç­–
    strategy = self._determine_adaptive_strategy(cognitive_load, confusion_level, knowledge_level)
    
    questions = []
    
    for i in range(num_questions):
        # 3. ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹ç”Ÿæˆé¢˜ç›®
        target_kp = target_knowledge_points[i % len(target_knowledge_points)]
        
        # 4. é€‰æ‹©é¢˜ç›®ç±»å‹
        question_type = self._select_question_type(strategy, target_kp)
        
        # 5. ç¡®å®šéš¾åº¦ç­‰çº§
        difficulty = self._determine_difficulty(strategy, knowledge_level)
        
        # 6. ç”Ÿæˆå…·ä½“é¢˜ç›®
        question = self._generate_question(question_type, target_kp, difficulty, strategy)
        
        if question:
            questions.append(question)
    
    return questions

def _determine_adaptive_strategy(self, cognitive_load: str, 
                               confusion_level: str, 
                               knowledge_level: float) -> Dict[str, Any]:
    """
    è‡ªé€‚åº”ç­–ç•¥å†³ç­–ç®—æ³•
    
    ç­–ç•¥çŸ©é˜µï¼š
    - é«˜è®¤çŸ¥è´Ÿè· + é«˜å›°æƒ‘ â†’ ç®€åŒ–é¢˜ç›®ï¼Œæä¾›æ›´å¤šæç¤º
    - ä½è®¤çŸ¥è´Ÿè· + ä½å›°æƒ‘ â†’ æŒ‘æˆ˜æ€§é¢˜ç›®ï¼Œå‡å°‘æç¤º
    - ä¸­ç­‰çŠ¶æ€ â†’ æ ‡å‡†é¢˜ç›®
    """
    
    strategy = {
        'provide_hints': True,
        'reduce_complexity': False,
        'increase_challenge': False,
        'focus_on_basics': False,
        'encourage_exploration': False
    }
    
    # è®¤çŸ¥è´Ÿè·é€‚åº”
    if cognitive_load == 'high':
        strategy['reduce_complexity'] = True
        strategy['provide_hints'] = True
        strategy['focus_on_basics'] = True
    elif cognitive_load == 'low':
        strategy['increase_challenge'] = True
        strategy['encourage_exploration'] = True
        strategy['provide_hints'] = False
    
    # å›°æƒ‘ç¨‹åº¦é€‚åº”
    if confusion_level in ['moderate', 'severe']:
        strategy['reduce_complexity'] = True
        strategy['provide_hints'] = True
        strategy['focus_on_basics'] = True
    
    # çŸ¥è¯†æ°´å¹³é€‚åº”
    if knowledge_level < 2.0:
        strategy['focus_on_basics'] = True
        strategy['provide_hints'] = True
    elif knowledge_level > 4.0:
        strategy['increase_challenge'] = True
        strategy['encourage_exploration'] = True
    
    return strategy

def _select_question_type(self, strategy: Dict[str, Any], 
                         knowledge_point: str) -> QuestionType:
    """
    åŸºäºç­–ç•¥é€‰æ‹©é¢˜ç›®ç±»å‹
    
    é€‰æ‹©é€»è¾‘ï¼š
    - åŸºç¡€å·©å›ºç­–ç•¥ â†’ å¡«ç©ºé¢˜
    - æŒ‘æˆ˜ç­–ç•¥ â†’ å®ç°é¢˜  
    - æ ‡å‡†ç­–ç•¥ â†’ æ··åˆç±»å‹
    """
    
    if strategy.get('focus_on_basics', False):
        # åŸºç¡€å·©å›ºï¼šé€‰æ‹©å¡«ç©ºé¢˜
        return QuestionType.FILL_IN_BLANK
    elif strategy.get('increase_challenge', False):
        # å¢åŠ æŒ‘æˆ˜ï¼šé€‰æ‹©å®ç°é¢˜
        return QuestionType.CODE_IMPLEMENTATION
    elif strategy.get('reduce_complexity', False):
        # é™ä½å¤æ‚åº¦ï¼šé€‰æ‹©é”™è¯¯çº æ­£é¢˜
        return QuestionType.ERROR_CORRECTION
    else:
        # æ ‡å‡†æƒ…å†µï¼šéšæœºé€‰æ‹©
        return random.choice([
            QuestionType.FILL_IN_BLANK,
            QuestionType.ERROR_CORRECTION,
            QuestionType.CODE_IMPLEMENTATION
        ])

def _generate_fill_in_blank_question(self, knowledge_point: str,
                                   difficulty: DifficultyLevel,
                                   strategy: Dict[str, Any]) -> Question:
    """
    ç”Ÿæˆå¡«ç©ºé¢˜
    
    ç®—æ³•ï¼š
    1. é€‰æ‹©ä»£ç æ¨¡æ¿
    2. è¯†åˆ«å…³é”®ä½ç½®
    3. ç”Ÿæˆç©ºç™½
    4. æ·»åŠ æç¤º
    """
    
    # æ ¹æ®çŸ¥è¯†ç‚¹é€‰æ‹©æ¨¡æ¿
    templates = self.question_templates.get(knowledge_point, {})
    difficulty_templates = templates.get(difficulty.value, [])
    
    if not difficulty_templates:
        return None
    
    # éšæœºé€‰æ‹©æ¨¡æ¿
    template_data = random.choice(difficulty_templates)
    
    # ç”Ÿæˆä»£ç æ¨¡æ¿
    code_template = template_data['code']
    blanks = template_data['blanks']
    
    # æ ¹æ®ç­–ç•¥è°ƒæ•´æç¤º
    hints = template_data.get('hints', [])
    if not strategy.get('provide_hints', True):
        hints = []  # ä¸æä¾›æç¤º
    
    # åˆ›å»ºé¢˜ç›®
    question = Question(
        id=f"fill_{knowledge_point}_{int(time.time())}",
        type=QuestionType.FILL_IN_BLANK,
        title=f"{knowledge_point} - ä»£ç å¡«ç©ºé¢˜",
        content=template_data.get('description', 'å®Œæˆä¸‹é¢çš„ä»£ç '),
        knowledge_points=[knowledge_point],
        difficulty=difficulty,
        estimated_time=3 if difficulty == DifficultyLevel.EASY else 5,
        max_score=100,
        template=CodeTemplate(
            template=code_template,
            blanks=blanks,
            hints=hints
        )
    )
    
    return question
```

### 5.3 è‡ªåŠ¨è¯„ä¼°ç®—æ³•

#### å¡«ç©ºé¢˜è¯„ä¼°
```python
def evaluate_fill_in_blank(self, question: Question, 
                          user_answers: List[str]) -> Dict[str, Any]:
    """
    å¡«ç©ºé¢˜è‡ªåŠ¨è¯„ä¼°
    
    ç®—æ³•ï¼š
    1. ç­”æ¡ˆåŒ¹é… (ç²¾ç¡®åŒ¹é… + æ¨¡ç³ŠåŒ¹é…)
    2. è¯­æ³•æ£€æŸ¥
    3. è¯­ä¹‰åˆ†æ
    4. è¯„åˆ†è®¡ç®—
    """
    
    if not question.template or not question.template.blanks:
        return {'score': 0, 'feedback': 'é¢˜ç›®é…ç½®é”™è¯¯'}
    
    total_blanks = len(question.template.blanks)
    correct_count = 0
    detailed_feedback = []
    
    for i, (blank_info, user_answer) in enumerate(zip(question.template.blanks, user_answers)):
        expected_answers = blank_info.get('expected', [])
        blank_type = blank_info.get('type', 'exact')
        
        # ç­”æ¡ˆè¯„ä¼°
        is_correct, feedback = self._evaluate_single_blank(
            user_answer, expected_answers, blank_type
        )
        
        if is_correct:
            correct_count += 1
            detailed_feedback.append(f"ç©ºç™½ {i+1}: âœ“ æ­£ç¡®")
        else:
            detailed_feedback.append(f"ç©ºç™½ {i+1}: âœ— {feedback}")
    
    # è®¡ç®—åˆ†æ•°
    accuracy = correct_count / total_blanks
    score = accuracy * question.max_score
    
    # ç”Ÿæˆç»¼åˆåé¦ˆ
    if accuracy >= 0.8:
        overall_feedback = "è¡¨ç°å‡ºè‰²ï¼æŒæ¡å¾—å¾ˆå¥½ã€‚"
    elif accuracy >= 0.6:
        overall_feedback = "åŸºæœ¬æ­£ç¡®ï¼Œè¿˜æœ‰æå‡ç©ºé—´ã€‚"
    else:
        overall_feedback = "éœ€è¦ç»§ç»­å­¦ä¹ ï¼Œå»ºè®®å¤ä¹ ç›¸å…³æ¦‚å¿µã€‚"
    
    return {
        'score': score,
        'accuracy': accuracy,
        'feedback': overall_feedback,
        'detailed_feedback': detailed_feedback,
        'suggestions': self._generate_learning_suggestions(question, accuracy)
    }

def _evaluate_single_blank(self, user_answer: str, 
                          expected_answers: List[str],
                          blank_type: str) -> Tuple[bool, str]:
    """
    å•ä¸ªç©ºç™½è¯„ä¼°
    
    æ”¯æŒå¤šç§åŒ¹é…æ¨¡å¼ï¼š
    - exact: ç²¾ç¡®åŒ¹é…
    - flexible: çµæ´»åŒ¹é… (å¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼)
    - semantic: è¯­ä¹‰åŒ¹é…
    """
    
    user_answer = user_answer.strip()
    
    if blank_type == 'exact':
        # ç²¾ç¡®åŒ¹é…
        if user_answer in expected_answers:
            return True, "æ­£ç¡®"
        else:
            return False, f"åº”è¯¥æ˜¯: {'/'.join(expected_answers)}"
    
    elif blank_type == 'flexible':
        # çµæ´»åŒ¹é…
        normalized_user = user_answer.lower().replace(' ', '')
        normalized_expected = [ans.lower().replace(' ', '') for ans in expected_answers]
        
        if normalized_user in normalized_expected:
            return True, "æ­£ç¡®"
        else:
            return False, f"åº”è¯¥æ˜¯: {'/'.join(expected_answers)}"
    
    elif blank_type == 'semantic':
        # è¯­ä¹‰åŒ¹é… (ç®€åŒ–ç‰ˆ)
        return self._semantic_match(user_answer, expected_answers)
    
    else:
        return False, "æœªçŸ¥çš„åŒ¹é…ç±»å‹"

def _semantic_match(self, user_answer: str, 
                   expected_answers: List[str]) -> Tuple[bool, str]:
    """
    è¯­ä¹‰åŒ¹é…è¯„ä¼°
    
    ç®€åŒ–çš„è¯­ä¹‰ç†è§£ï¼š
    - åŒä¹‰è¯åŒ¹é…
    - åŠŸèƒ½ç­‰ä»·æ€§æ£€æŸ¥
    """
    
    # åŒä¹‰è¯æ˜ å°„
    synonyms = {
        'div': ['container', 'box'],
        'span': ['inline', 'text'],
        'class': ['className'],
        'id': ['identifier']
    }
    
    user_lower = user_answer.lower()
    
    for expected in expected_answers:
        expected_lower = expected.lower()
        
        # ç›´æ¥åŒ¹é…
        if user_lower == expected_lower:
            return True, "æ­£ç¡®"
        
        # åŒä¹‰è¯åŒ¹é…
        if expected_lower in synonyms:
            if user_lower in synonyms[expected_lower]:
                return True, "æ­£ç¡® (åŒä¹‰è¯)"
        
        # åå‘åŒä¹‰è¯åŒ¹é…
        for key, vals in synonyms.items():
            if expected_lower in vals and user_lower == key:
                return True, "æ­£ç¡® (åŒä¹‰è¯)"
    
    return False, f"è¯­ä¹‰ä¸åŒ¹é…ï¼ŒæœŸæœ›: {'/'.join(expected_answers)}"
```

## ğŸ”„ 6. å•ç”¨æˆ·æ·±åº¦å»ºæ¨¡ (single_user_model.py)

### 6.1 é›†æˆæ¶æ„
å•ç”¨æˆ·æ¨¡å‹æ˜¯æ‰€æœ‰åˆ†æç»„ä»¶çš„é›†æˆï¼Œå®ç°æ·±åº¦ä¸ªæ€§åŒ–å»ºæ¨¡ã€‚

### 6.2 å¤šæ¨¡å‹èåˆç®—æ³•

#### çŠ¶æ€èåˆç®—æ³•
```python
def update_from_behavior(self, behavior_data: Dict[str, Any],
                        session_summary: Dict[str, Any]) -> None:
    """
    å¤šæ¨¡å‹èåˆçš„çŠ¶æ€æ›´æ–°ç®—æ³•
    
    èåˆæµç¨‹ï¼š
    1. åŸºç¡€æ¨¡å‹æ›´æ–°
    2. BKTçŸ¥è¯†è¿½è¸ªæ›´æ–°  
    3. MLçŠ¶æ€é¢„æµ‹
    4. å¤šæ¨¡å‹ç½®ä¿¡åº¦åŠ æƒèåˆ
    """
    
    try:
        # 1. æ›´æ–°åŸºç¡€æ¨¡å‹
        self.base_model_service.update_from_behavior_data(
            self.student_id, self.session_id
        )
        
        # 2. æå–ç‰¹å¾ç”¨äºMLé¢„æµ‹
        feature_vector = self._extract_features(behavior_data, session_summary)
        
        # 3. MLæ¨¡å‹é¢„æµ‹
        ml_predictions = self.ml_predictor.predict_states(feature_vector)
        
        # 4. å¤šæ¨¡å‹èåˆ
        self._integrate_ml_predictions(ml_predictions)
        
        # 5. æ›´æ–°æ—¶é—´æˆ³
        self.last_update_time = time.time()
        
        # 6. å®šæœŸæŒä¹…åŒ–
        if self.last_update_time % 60 < 1:
            self._save_persistent_data()
        
        logger.info("å•ç”¨æˆ·æ¨¡å‹æ›´æ–°å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ›´æ–°å•ç”¨æˆ·æ¨¡å‹å¤±è´¥: {e}")

def _integrate_ml_predictions(self, ml_predictions: Dict[str, Any]) -> None:
    """
    MLé¢„æµ‹ç»“æœèåˆç®—æ³•
    
    èåˆç­–ç•¥ï¼š
    1. ç½®ä¿¡åº¦åŠ æƒ
    2. å†å²ä¸€è‡´æ€§æ£€æŸ¥
    3. å¼‚å¸¸å€¼è¿‡æ»¤
    """
    
    if not ml_predictions:
        return
    
    model = self.base_model_service.get_model(self.student_id)
    
    # è®¤çŸ¥è´Ÿè·èåˆ
    if 'cognitive_load' in ml_predictions:
        cog_pred = ml_predictions['cognitive_load']
        
        # åªæœ‰é«˜ç½®ä¿¡åº¦é¢„æµ‹æ‰åº”ç”¨
        if cog_pred.confidence > 0.7:
            # æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡èåˆ
            Î± = 0.3  # æ–°é¢„æµ‹æƒé‡
            old_load_score = self._load_to_score(model.cognitive_state.cognitive_load)
            new_load_score = self._load_to_score(cog_pred.prediction)
            
            fused_score = Î± * new_load_score + (1 - Î±) * old_load_score
            
            model.cognitive_state.cognitive_load = self._score_to_load(fused_score)
            model.cognitive_state.load_confidence = cog_pred.confidence
    
    # å›°æƒ‘ç¨‹åº¦èåˆ  
    if 'confusion' in ml_predictions:
        conf_pred = ml_predictions['confusion']
        
        if conf_pred.confidence > 0.7:
            Î± = 0.4  # å›°æƒ‘çŠ¶æ€å˜åŒ–è¾ƒå¿«ï¼Œæƒé‡ç•¥é«˜
            old_confusion = model.cognitive_state.confusion_score
            new_confusion = conf_pred.prediction
            
            fused_confusion = Î± * new_confusion + (1 - Î±) * old_confusion
            
            model.cognitive_state.confusion_score = fused_confusion
            model.cognitive_state.confusion_level = self._score_to_confusion_level(fused_confusion)
    
    logger.debug("MLé¢„æµ‹ç»“æœå·²èåˆåˆ°åŸºç¡€æ¨¡å‹")
```

#### ä¸ªæ€§åŒ–æ¨èç®—æ³•
```python
def get_personalized_recommendations(self) -> List[str]:
    """
    ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®ç”Ÿæˆç®—æ³•
    
    ç®—æ³•æµç¨‹ï¼š
    1. åˆ†æå½“å‰å­¦ä¹ çŠ¶æ€
    2. è¯†åˆ«å­¦ä¹ ç“¶é¢ˆå’Œä¼˜åŠ¿
    3. åŸºäºå­¦ä¹ åå¥½ç”Ÿæˆå»ºè®®
    4. ä¼˜å…ˆçº§æ’åº
    """
    
    base_summary = self.get_model_summary()
    recommendations = []
    
    # 1. è®¤çŸ¥è´Ÿè·å»ºè®®
    cognitive_load = base_summary.get('cognitive_state', {}).get('cognitive_load')
    if cognitive_load == 'high':
        recommendations.append({
            'text': "å½“å‰è®¤çŸ¥è´Ÿè·è¾ƒé«˜ï¼Œå»ºè®®é€‚å½“ä¼‘æ¯æˆ–é€‰æ‹©è¾ƒç®€å•çš„ç»ƒä¹ ",
            'priority': 'high',
            'category': 'cognitive_management'
        })
    elif cognitive_load == 'low':
        recommendations.append({
            'text': "å½“å‰çŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥å°è¯•æ›´æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡",
            'priority': 'medium',
            'category': 'challenge_increase'
        })
    
    # 2. å›°æƒ‘ç¨‹åº¦å»ºè®®
    confusion_level = base_summary.get('cognitive_state', {}).get('confusion_level')
    if confusion_level in ['moderate', 'severe']:
        recommendations.append({
            'text': "æ£€æµ‹åˆ°å­¦ä¹ å›°æƒ‘ï¼Œå»ºè®®å›é¡¾åŸºç¡€æ¦‚å¿µæˆ–å¯»æ±‚å¸®åŠ©",
            'priority': 'high',
            'category': 'confusion_resolution'
        })
    
    # 3. çŸ¥è¯†ç‚¹æŒæ¡å»ºè®®
    bkt_analysis = base_summary.get('bkt_analysis', {})
    struggling_count = bkt_analysis.get('struggling_count', 0)
    well_mastered_count = bkt_analysis.get('well_mastered_count', 0)
    
    if struggling_count > 0:
        recommendations.append({
            'text': f"æœ‰ {struggling_count} ä¸ªçŸ¥è¯†ç‚¹éœ€è¦åŠ å¼ºç»ƒä¹ ",
            'priority': 'high',
            'category': 'knowledge_reinforcement'
        })
    
    if well_mastered_count > 0:
        recommendations.append({
            'text': f"å·²ç»å¾ˆå¥½æŒæ¡äº† {well_mastered_count} ä¸ªçŸ¥è¯†ç‚¹ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ",
            'priority': 'medium',
            'category': 'progress_advancement'
        })
    
    # 4. å­¦ä¹ åå¥½å»ºè®®
    main_preference = base_summary.get('learning_preferences', {}).get('main_preference')
    if main_preference == 'code_examples':
        recommendations.append({
            'text': "å»ºè®®é€šè¿‡æ›´å¤šä»£ç ç¤ºä¾‹æ¥å­¦ä¹ ",
            'priority': 'low',
            'category': 'learning_method'
        })
    elif main_preference == 'text_explanations':
        recommendations.append({
            'text': "å»ºè®®å¤šé˜…è¯»è¯¦ç»†çš„æ¦‚å¿µè§£é‡Š",
            'priority': 'low',
            'category': 'learning_method'
        })
    
    # 5. ä¼˜å…ˆçº§æ’åºå’Œå»é‡
    recommendations.sort(key=lambda x: {'high': 3, 'medium': 2, 'low': 1}[x['priority']], reverse=True)
    
    # è¿”å›å‰5æ¡å»ºè®®çš„æ–‡æœ¬
    return [rec['text'] for rec in recommendations[:5]]
```

## ğŸ“ˆ 7. ç¦»çº¿è¯„ä¼°ç³»ç»Ÿ (offline_evaluator.py)

### 7.1 è¯„ä¼°æ¡†æ¶
ä¸ºå­¦æœ¯ç ”ç©¶æä¾›æ¨¡å‹å‡†ç¡®æ€§éªŒè¯å’Œæ•ˆæœè¯„ä¼°ã€‚

### 7.2 æ¨¡å‹å‡†ç¡®æ€§è¯„ä¼°

#### BKTæ¨¡å‹è¯„ä¼°ç®—æ³•
```python
def evaluate_bkt_accuracy(self) -> EvaluationMetrics:
    """
    BKTæ¨¡å‹å‡†ç¡®æ€§è¯„ä¼°
    
    è¯„ä¼°æŒ‡æ ‡ï¼š
    1. æŒæ¡çŠ¶æ€é¢„æµ‹å‡†ç¡®ç‡
    2. é¢„æµ‹å€¼ä¸çœŸå®å€¼ç›¸å…³æ€§
    3. å‡æ–¹è¯¯å·® (MSE)
    """
    
    if not self.ground_truth_data or not self.model_predictions:
        return EvaluationMetrics()
    
    # åŒ¹é…çœŸå®æ ‡ç­¾å’Œé¢„æµ‹
    true_mastery = []
    pred_mastery = []
    
    for gt in self.ground_truth_data:
        # æŸ¥æ‰¾å¯¹åº”çš„BKTé¢„æµ‹
        matching_pred = self._find_matching_prediction(gt, 'bkt_mastery')
        if matching_pred:
            true_mastery.append(gt.true_mastery_level)
            pred_mastery.append(matching_pred.get('predicted_mastery', 0.5))
    
    if not true_mastery:
        return EvaluationMetrics()
    
    # è®¡ç®—å›å½’æŒ‡æ ‡
    mse = mean_squared_error(true_mastery, pred_mastery)
    correlation = np.corrcoef(true_mastery, pred_mastery)[0, 1] if len(true_mastery) > 1 else 0.0
    
    # è½¬æ¢ä¸ºäºŒåˆ†ç±»å‡†ç¡®ç‡ (æŒæ¡ vs æœªæŒæ¡)
    threshold = 0.6
    true_binary = [1 if m > threshold else 0 for m in true_mastery]
    pred_binary = [1 if m > threshold else 0 for m in pred_mastery]
    
    accuracy = accuracy_score(true_binary, pred_binary)
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_binary, pred_binary, average='binary', zero_division=0
    )
    
    return EvaluationMetrics(
        accuracy=accuracy,
        precision=precision,
        recall=recall,
        f1_score=f1,
        mse=mse,
        correlation=correlation,
        sample_size=len(true_mastery)
    )
```

#### å¯¹ç…§å®éªŒæ•°æ®åˆ†æ
```python
def collect_comparative_study_data(self, control_group_data: List[Dict],
                                 experimental_group_data: List[Dict]) -> Dict[str, Any]:
    """
    å¯¹ç…§å®éªŒæ•°æ®æ”¶é›†å’Œåˆ†æ
    
    åˆ†æç»´åº¦ï¼š
    1. å­¦ä¹ æ•ˆç‡ (å®Œæˆæ—¶é—´)
    2. å­¦ä¹ æ•ˆæœ (å‡†ç¡®ç‡)  
    3. ç”¨æˆ·ä½“éªŒ (æ»¡æ„åº¦)
    4. è®¤çŸ¥è´Ÿè·
    """
    
    # åˆ†æå¯¹ç…§ç»„
    control_analysis = self._analyze_group_performance(control_group_data, "é™æ€AI")
    
    # åˆ†æå®éªŒç»„  
    experimental_analysis = self._analyze_group_performance(experimental_group_data, "åŠ¨æ€AI")
    
    # æ¯”è¾ƒåˆ†æ
    comparison = self._compare_groups(control_analysis, experimental_analysis)
    
    return {
        'study_type': 'comparative_effectiveness',
        'control_group': control_analysis,
        'experimental_group': experimental_analysis,
        'comparison_results': comparison,
        'statistical_significance': self._calculate_significance(
            control_group_data, experimental_group_data
        ),
        'paper_ready_metrics': self._generate_paper_metrics(comparison)
    }

def _analyze_group_performance(self, group_data: List[Dict], group_name: str) -> Dict[str, Any]:
    """ç»„åˆ«æ€§èƒ½åˆ†æ"""
    if not group_data:
        return {}
    
    # å­¦ä¹ æ•ˆæœæŒ‡æ ‡
    completion_times = [d.get('completion_time', 0) for d in group_data]
    accuracy_scores = [d.get('accuracy', 0) for d in group_data]
    satisfaction_scores = [d.get('satisfaction', 0) for d in group_data]
    cognitive_load_scores = [d.get('cognitive_load_score', 0) for d in group_data]
    
    return {
        'group_name': group_name,
        'sample_size': len(group_data),
        'learning_efficiency': {
            'avg_completion_time': np.mean(completion_times),
            'completion_time_std': np.std(completion_times),
            'completion_time_median': np.median(completion_times)
        },
        'learning_effectiveness': {
            'avg_accuracy': np.mean(accuracy_scores),
            'accuracy_std': np.std(accuracy_scores),
            'accuracy_median': np.median(accuracy_scores)
        },
        'user_experience': {
            'avg_satisfaction': np.mean(satisfaction_scores),
            'satisfaction_std': np.std(satisfaction_scores)
        },
        'cognitive_load': {
            'avg_load': np.mean(cognitive_load_scores),
            'load_std': np.std(cognitive_load_scores)
        }
    }

def _compare_groups(self, control: Dict, experimental: Dict) -> Dict[str, Any]:
    """ç»„é—´æ¯”è¾ƒåˆ†æ"""
    if not control or not experimental:
        return {}
    
    improvements = {}
    
    # å­¦ä¹ æ•ˆç‡æ”¹è¿› (æ—¶é—´è¶ŠçŸ­è¶Šå¥½)
    if control.get('learning_efficiency', {}).get('avg_completion_time', 0) > 0:
        time_improvement = (
            (control['learning_efficiency']['avg_completion_time'] - 
             experimental['learning_efficiency']['avg_completion_time']) /
            control['learning_efficiency']['avg_completion_time'] * 100
        )
        improvements['completion_time'] = time_improvement
    
    # å­¦ä¹ æ•ˆæœæ”¹è¿› (å‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½)
    if control.get('learning_effectiveness', {}).get('avg_accuracy', 0) > 0:
        accuracy_improvement = (
            (experimental['learning_effectiveness']['avg_accuracy'] - 
             control['learning_effectiveness']['avg_accuracy']) /
            control['learning_effectiveness']['avg_accuracy'] * 100
        )
        improvements['accuracy'] = accuracy_improvement
    
    # ç”¨æˆ·æ»¡æ„åº¦æ”¹è¿›
    if control.get('user_experience', {}).get('avg_satisfaction', 0) > 0:
        satisfaction_improvement = (
            (experimental['user_experience']['avg_satisfaction'] - 
             control['user_experience']['avg_satisfaction']) /
            control['user_experience']['avg_satisfaction'] * 100
        )
        improvements['satisfaction'] = satisfaction_improvement
    
    # è®¤çŸ¥è´Ÿè·æ”¹è¿› (è´Ÿè·è¶Šä½è¶Šå¥½)
    if control.get('cognitive_load', {}).get('avg_load', 0) > 0:
        load_improvement = (
            (control['cognitive_load']['avg_load'] - 
             experimental['cognitive_load']['avg_load']) /
            control['cognitive_load']['avg_load'] * 100
        )
        improvements['cognitive_load'] = load_improvement
    
    return {
        'improvements': improvements,
        'summary': self._generate_comparison_summary(improvements),
        'effect_sizes': self._calculate_effect_sizes(control, experimental)
    }
```

## ğŸ”Œ 8. APIé›†æˆæœåŠ¡ (api_integration.py)

### 8.1 æœåŠ¡å°è£…
å°†æ‰€æœ‰åˆ†æåŠŸèƒ½å°è£…ä¸ºRESTful APIï¼Œæ”¯æŒå‰ç«¯é›†æˆå’Œç¬¬ä¸‰æ–¹è°ƒç”¨ã€‚

### 8.2 æ ¸å¿ƒAPIå®ç°

#### å­¦ä¹ è€…æ¨¡å‹API
```python
async def get_student_model_summary(self, student_id: str) -> Dict[str, Any]:
    """
    è·å–å¢å¼ºå­¦ä¹ è€…æ¨¡å‹æ‘˜è¦
    
    è¿”å›å®Œæ•´çš„å¤šç»´åº¦å­¦ä¹ è€…ç”»åƒï¼ŒåŒ…æ‹¬ï¼š
    1. åŸºç¡€è®¤çŸ¥å’Œæƒ…æ„ŸçŠ¶æ€
    2. BKTçŸ¥è¯†è¿½è¸ªåˆ†æ
    3. MLæ¨¡å‹çŠ¶æ€å’Œé¢„æµ‹
    4. ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®
    """
    
    if not self.single_user_model:
        raise HTTPException(status_code=503, detail="å•ç”¨æˆ·å­¦ä¹ æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
    
    # ä½¿ç”¨å•ç”¨æˆ·æ¨¡å‹è·å–å¢å¼ºæ‘˜è¦
    summary = self.single_user_model.get_model_summary()
    
    # æ·»åŠ APIç‰ˆæœ¬ä¿¡æ¯å’ŒåŠŸèƒ½æ ‡è¯†
    summary.update({
        'api_version': 'v2_single_user',
        'enhanced_features': [
            'bayesian_knowledge_tracking',
            'ml_state_prediction', 
            'single_user_optimization',
            'persistent_learning_data',
            'personalized_recommendations'
        ],
        'model_confidence': self._calculate_overall_confidence(summary),
        'last_interaction': time.time()
    })
    
    return summary

def _calculate_overall_confidence(self, summary: Dict[str, Any]) -> float:
    """
    è®¡ç®—æ¨¡å‹æ•´ä½“ç½®ä¿¡åº¦
    
    ç»¼åˆè€ƒè™‘ï¼š
    1. æ•°æ®æ ·æœ¬æ•°é‡
    2. æ¨¡å‹è®­ç»ƒçŠ¶æ€  
    3. é¢„æµ‹ä¸€è‡´æ€§
    """
    
    confidence_factors = []
    
    # æ•°æ®é‡ç½®ä¿¡åº¦
    total_interactions = summary.get('interaction_count', 0)
    data_confidence = min(total_interactions / 100, 1.0)  # 100æ¬¡äº¤äº’è¾¾åˆ°æ»¡ä¿¡å¿ƒ
    confidence_factors.append(data_confidence * 0.4)
    
    # æ¨¡å‹è®­ç»ƒç½®ä¿¡åº¦
    ml_status = summary.get('ml_model_status', {})
    ml_confidence = 1.0 if ml_status.get('cognitive_load_trained', False) else 0.5
    confidence_factors.append(ml_confidence * 0.3)
    
    # BKTç½®ä¿¡åº¦
    bkt_analysis = summary.get('bkt_analysis', {})
    avg_mastery = bkt_analysis.get('average_mastery', 0.1)
    bkt_confidence = min(avg_mastery * 2, 1.0)  # æŒæ¡åº¦è¶Šé«˜ç½®ä¿¡åº¦è¶Šé«˜
    confidence_factors.append(bkt_confidence * 0.3)
    
    return sum(confidence_factors)
```

#### è‡ªé€‚åº”å‡ºé¢˜API
```python
async def generate_adaptive_quiz(self, request: QuizGenerationRequest) -> Dict[str, Any]:
    """
    ç”Ÿæˆè‡ªé€‚åº”æµ‹è¯•é¢˜API
    
    APIæµç¨‹ï¼š
    1. è·å–å­¦ä¹ è€…å½“å‰çŠ¶æ€
    2. è°ƒç”¨è‡ªé€‚åº”å‡ºé¢˜ç®—æ³•
    3. ç¼“å­˜é¢˜ç›®ç”¨äºåç»­è¯„ä¼°
    4. è¿”å›ç»“æ„åŒ–é¢˜ç›®æ•°æ®
    """
    
    if not self.quiz_generator or not self.student_model_service:
        raise HTTPException(status_code=503, detail="å‡ºé¢˜æœåŠ¡ä¸å¯ç”¨")
    
    # è·å–å­¦ä¹ è€…æ¨¡å‹
    student_model_summary = self.student_model_service.get_model_summary(request.student_id)
    
    # ç”Ÿæˆè‡ªé€‚åº”é¢˜ç›®
    questions = self.quiz_generator.generate_adaptive_quiz(
        student_model_summary=student_model_summary,
        target_knowledge_points=request.knowledge_points,
        num_questions=request.num_questions
    )
    
    # ç¼“å­˜é¢˜ç›®ç”¨äºåç»­è¯„ä¼°
    for question in questions:
        self.question_cache[question.id] = question
    
    # è½¬æ¢ä¸ºAPIå“åº”æ ¼å¼
    questions_data = []
    for question in questions:
        question_data = {
            'id': question.id,
            'type': question.type.value,
            'title': question.title,
            'content': question.content,
            'knowledge_points': question.knowledge_points,
            'difficulty': question.difficulty.value,
            'estimated_time': question.estimated_time,
            'max_score': question.max_score,
            'adaptive_metadata': {
                'generated_for_cognitive_load': student_model_summary['cognitive_state']['cognitive_load'],
                'generated_for_confusion_level': student_model_summary['cognitive_state']['confusion_level'],
                'adaptation_strategy_applied': True
            }
        }
        
        # æ ¹æ®é¢˜å‹æ·»åŠ ç‰¹å®šæ•°æ®
        if question.type == QuestionType.FILL_IN_BLANK and question.template:
            question_data['template'] = {
                'code_template': question.template.template,
                'blank_count': len(question.template.blanks),
                'hints': question.template.hints if student_model_summary['cognitive_state']['cognitive_load'] == 'high' else []
            }
        
        questions_data.append(question_data)
    
    return {
        'status': 'success',
        'questions': questions_data,
        'total_questions': len(questions_data),
        'generation_metadata': {
            'student_knowledge_level': student_model_summary['cognitive_state']['knowledge_level'],
            'cognitive_load': student_model_summary['cognitive_state']['cognitive_load'],
            'confusion_level': student_model_summary['cognitive_state']['confusion_level'],
            'adaptive_adjustments_applied': True,
            'generation_timestamp': time.time()
        }
    }
```

## ğŸ¯ å…³é”®ç®—æ³•æ€»ç»“

### 1. æ ¸å¿ƒåˆ›æ–°ç®—æ³•
- **æ§åˆ¶è®ºé—­ç¯**: ä¼ æ„Ÿå™¨â†’æ§åˆ¶å™¨â†’æ‰§è¡Œå™¨â†’åé¦ˆ
- **å¤šæ¨¡å‹èåˆ**: BKT + ML + è§„åˆ™ç³»ç»Ÿçš„ç½®ä¿¡åº¦åŠ æƒèåˆ
- **è‡ªé€‚åº”ç­–ç•¥**: åŸºäºè®¤çŸ¥è´Ÿè·å’Œå›°æƒ‘ç¨‹åº¦çš„åŠ¨æ€æ•™å­¦ç­–ç•¥

### 2. æœºå™¨å­¦ä¹ ç®—æ³•
- **ç‰¹å¾å·¥ç¨‹**: 19ç»´è¡Œä¸ºç‰¹å¾æå–
- **Random Forest**: è®¤çŸ¥è´Ÿè·å’Œå›°æƒ‘ç¨‹åº¦é¢„æµ‹
- **æ—¶é—´åºåˆ—åˆ†æ**: å­¦ä¹ è½¨è¿¹å’Œè¶‹åŠ¿é¢„æµ‹

### 3. çŸ¥è¯†è¿½è¸ªç®—æ³•
- **æ ‡å‡†BKT**: è´å¶æ–¯çŸ¥è¯†è¿½è¸ªå››å‚æ•°æ¨¡å‹
- **å‚æ•°ä¼°è®¡**: EMç®—æ³•å’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡
- **å‰å‘-åå‘ç®—æ³•**: éšçŠ¶æ€æ¦‚ç‡è®¡ç®—

### 4. è¯„ä¼°éªŒè¯ç®—æ³•
- **ç¦»çº¿è¯„ä¼°**: æ¨¡å‹å‡†ç¡®æ€§éªŒè¯
- **å¯¹ç…§å®éªŒ**: æ•ˆæœæ¯”è¾ƒå’Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
- **CHIè®ºæ–‡æŒ‡æ ‡**: å­¦æœ¯ç ”ç©¶å°±ç»ªçš„è¯„ä¼°æ¡†æ¶

è¿™å¥—analyticsæ¨¡å—ä¸ºæ‚¨çš„CHIè®ºæ–‡æä¾›äº†å®Œæ•´çš„æŠ€æœ¯æ”¯æ’‘ï¼Œå®ç°äº†çœŸæ­£çš„"äºº-LLMèåˆ"æ™ºèƒ½æ•™å­¦ç³»ç»Ÿã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 æŠ€æœ¯è¯¦è§£  
**é€‚ç”¨äº**: ACM CHIä¼šè®®è®ºæ–‡  
**æŠ€æœ¯æ ˆ**: Python + scikit-learn + FastAPI + è´å¶æ–¯ç»Ÿè®¡å­¦